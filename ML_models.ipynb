{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udacity course \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Softmax function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(-2,6,.1) ##similiar to linspace \n",
    "score  = np.vstack([x, np.ones_like(x), .2*np.ones_like(x)])  ##stacks the array vetically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.00000000e+00,  -1.90000000e+00,  -1.80000000e+00,\n",
       "         -1.70000000e+00,  -1.60000000e+00,  -1.50000000e+00,\n",
       "         -1.40000000e+00,  -1.30000000e+00,  -1.20000000e+00,\n",
       "         -1.10000000e+00,  -1.00000000e+00,  -9.00000000e-01,\n",
       "         -8.00000000e-01,  -7.00000000e-01,  -6.00000000e-01,\n",
       "         -5.00000000e-01,  -4.00000000e-01,  -3.00000000e-01,\n",
       "         -2.00000000e-01,  -1.00000000e-01,   1.77635684e-15,\n",
       "          1.00000000e-01,   2.00000000e-01,   3.00000000e-01,\n",
       "          4.00000000e-01,   5.00000000e-01,   6.00000000e-01,\n",
       "          7.00000000e-01,   8.00000000e-01,   9.00000000e-01,\n",
       "          1.00000000e+00,   1.10000000e+00,   1.20000000e+00,\n",
       "          1.30000000e+00,   1.40000000e+00,   1.50000000e+00,\n",
       "          1.60000000e+00,   1.70000000e+00,   1.80000000e+00,\n",
       "          1.90000000e+00,   2.00000000e+00,   2.10000000e+00,\n",
       "          2.20000000e+00,   2.30000000e+00,   2.40000000e+00,\n",
       "          2.50000000e+00,   2.60000000e+00,   2.70000000e+00,\n",
       "          2.80000000e+00,   2.90000000e+00,   3.00000000e+00,\n",
       "          3.10000000e+00,   3.20000000e+00,   3.30000000e+00,\n",
       "          3.40000000e+00,   3.50000000e+00,   3.60000000e+00,\n",
       "          3.70000000e+00,   3.80000000e+00,   3.90000000e+00,\n",
       "          4.00000000e+00,   4.10000000e+00,   4.20000000e+00,\n",
       "          4.30000000e+00,   4.40000000e+00,   4.50000000e+00,\n",
       "          4.60000000e+00,   4.70000000e+00,   4.80000000e+00,\n",
       "          4.90000000e+00,   5.00000000e+00,   5.10000000e+00,\n",
       "          5.20000000e+00,   5.30000000e+00,   5.40000000e+00,\n",
       "          5.50000000e+00,   5.60000000e+00,   5.70000000e+00,\n",
       "          5.80000000e+00,   5.90000000e+00],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00,   1.00000000e+00],\n",
       "       [  2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01,   2.00000000e-01,\n",
       "          2.00000000e-01,   2.00000000e-01]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax func--> generates a prb for the clases labels provided--> *10 /10 has differetn affects on the prb\n",
    "\n",
    "Hot code --> converts the number into bianary\n",
    "if you have too many classes one hot encoding can get a littel messy --> embeding comes in handy tho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df.hea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the library with the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load scikit's random forest classifier library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load numpy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Location = r'/Users/akash/Desktop/context_time_taken.csv'\n",
    "df = pd.read_csv(Location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Location, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276, 4)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['context_meta_id','time','date','day']]\n",
    "X_arr = pd.DataFrame.as_matrix(X).ravel()\n",
    "X = np.array(X)\n",
    "X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df[['late_ontime']]\n",
    "y_arr = pd.DataFrame.as_matrix(y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276,)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_arr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1104,)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2).fit(X, y_arr)\n",
    "clf2 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n",
    "clf3 = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X[275 :]\n",
    "X_test\n",
    "y_test = y_arr[275 :]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-8e8f75481eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/akash/anaconda/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'soft'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mmaj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/akash/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y_arr)\n",
    "scores2 = cross_val_score(clf2, X, y_arr)\n",
    "scores3 = cross_val_score(clf3, X, y_arr)\n",
    "\n",
    "scores.mean()\n",
    "Clf.fit()\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89597942964001875"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.13) [Logistic Regression]\n",
      "Accuracy: 0.90 (+/- 0.13) [Random Forest]\n",
      "Accuracy: 0.90 (+/- 0.13) [naive Bayes]\n",
      "Accuracy: 0.90 (+/- 0.13) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y_arr, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.93 (+/- 0.05) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.05) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"DNNRegressor with custom input_fn for Housing dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "COLUMNS = [\"context_meta_id\", \"time\", \"date\", \"day\"]\n",
    "FEATURES = [\"context_meta_id\", \"date\", \"day\"]\n",
    "LABEL = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context_meta_id', 'date', 'day']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "  # Feature cols\n",
    " \n",
    "def input_fn(data_set):\n",
    "  feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "  labels = tf.constant(data_set[LABEL].values)\n",
    "  return feature_cols, labels  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets\n",
    "training_set = pd.read_csv(\"boston_train.csv\", skipinitialspace=Tru3e,\n",
    "                         skiprows=1, names=COLUMNS)\n",
    "test_set = pd.read_csv(\"boston_test.csv\", skipinitialspace=True,\n",
    "                     skiprows=1, names=COLUMNS)\n",
    "# Set of 6 examples for which to predict median house values\n",
    "prediction_set = pd.read_csv(\"boston_predict.csv\", skipinitialspace=True,\n",
    "                               skiprows=1, names=COLUMNS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_cols = [tf.contrib.layers.real_valued_column(k)\n",
    "                  for k in FEATURES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1212b78d0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/context_model'}\n"
     ]
    }
   ],
   "source": [
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "regressor = tf.contrib.learn.DNNRegressor(feature_columns=feature_cols,\n",
    "                                        hidden_units=[4, 4],\n",
    "                                        model_dir=\"/tmp/context_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\n",
      "<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/Gather:0' shape=(?,) dtype=string>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "['File \"/Users/akash/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\\n    \"__main__\", mod_spec)', 'File \"/Users/akash/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\\n    exec(code, run_globals)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\\n    app.launch_new_instance()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\\n    app.start()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\\n    ioloop.IOLoop.instance().start()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\\n    super(ZMQIOLoop, self).start()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\\n    handler_func(fd_obj, events)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\\n    return fn(*args, **kwargs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\\n    self._handle_recv()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\\n    self._run_callback(callback, msg)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\\n    callback(*args, **kwargs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\\n    return fn(*args, **kwargs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\\n    return self.dispatch_shell(stream, msg)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\\n    handler(stream, idents, msg)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\\n    user_expressions, allow_stdin)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\\n    res = shell.run_cell(code, store_history=store_history, silent=silent)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\\n    interactivity=interactivity, compiler=compiler, result=result)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\\n    if self.run_code(code, result):', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)', 'File \"<ipython-input-275-1dfd2d664c59>\", line 2, in <module>\\n    regressor.fit(input_fn=lambda: input_fn(training_set), steps=5000)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\\n    return func(*args, **kwargs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 455, in fit\\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1003, in _train_model\\n    config=self._session_config', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 352, in MonitoredTrainingSession\\n    stop_grace_period_secs=stop_grace_period_secs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 648, in __init__\\n    stop_grace_period_secs=stop_grace_period_secs)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 477, in __init__\\n    self._sess = _RecoverableSession(self._coordinated_creator)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 822, in __init__\\n    _WrappedSession.__init__(self, self._create_session())', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 827, in _create_session\\n    return self._sess_creator.create_session()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 538, in create_session\\n    self.tf_sess = self._session_creator.create_session()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 403, in create_session\\n    self._scaffold.finalize()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 192, in finalize\\n    default_ready_for_local_init_op)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 254, in get_or_default\\n    op = default_constructor()', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 189, in default_ready_for_local_init_op\\n    variables.global_variables())', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 170, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 139, in _add_should_use_warning\\n    wrapped = TFShouldUseWarningWrapper(x)', 'File \"/Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 96, in __init__\\n    stack = [s.strip() for s in traceback.format_stack()]']\n",
      "==================================\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/context_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.57158e+07, step = 1\n",
      "INFO:tensorflow:global_step/sec: 883.955\n",
      "INFO:tensorflow:loss = 29250.8, step = 101 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 1028.79\n",
      "INFO:tensorflow:loss = 1486.33, step = 201 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1048.92\n",
      "INFO:tensorflow:loss = 81.5517, step = 301 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 984.893\n",
      "INFO:tensorflow:loss = 5.84354, step = 401 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1231.65\n",
      "INFO:tensorflow:loss = 1.74886, step = 501 (0.079 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 1120.36\n",
      "INFO:tensorflow:loss = 1.52734, step = 601 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 971.968\n",
      "INFO:tensorflow:loss = 1.51536, step = 701 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 920.761\n",
      "INFO:tensorflow:loss = 1.51471, step = 801 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 1325.94\n",
      "INFO:tensorflow:loss = 1.51467, step = 901 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1014.23\n",
      "INFO:tensorflow:loss = 1.51466, step = 1001 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1265.95\n",
      "INFO:tensorflow:loss = 1.51466, step = 1101 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1114.06\n",
      "INFO:tensorflow:loss = 1.51465, step = 1201 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1288.34\n",
      "INFO:tensorflow:loss = 1.51465, step = 1301 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1063.93\n",
      "INFO:tensorflow:loss = 1.51464, step = 1401 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1166.24\n",
      "INFO:tensorflow:loss = 1.51464, step = 1501 (0.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 1077.5\n",
      "INFO:tensorflow:loss = 1.51463, step = 1601 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1022.25\n",
      "INFO:tensorflow:loss = 1.51463, step = 1701 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 989.767\n",
      "INFO:tensorflow:loss = 1.51463, step = 1801 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 1036.3\n",
      "INFO:tensorflow:loss = 1.51462, step = 1901 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1061.44\n",
      "INFO:tensorflow:loss = 1.51462, step = 2001 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1054.92\n",
      "INFO:tensorflow:loss = 1.51461, step = 2101 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1056.74\n",
      "INFO:tensorflow:loss = 1.51461, step = 2201 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1099.84\n",
      "INFO:tensorflow:loss = 1.5146, step = 2301 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1083.66\n",
      "INFO:tensorflow:loss = 1.5146, step = 2401 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1019.71\n",
      "INFO:tensorflow:loss = 1.5146, step = 2501 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1057.87\n",
      "INFO:tensorflow:loss = 1.51459, step = 2601 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1056.33\n",
      "INFO:tensorflow:loss = 1.51459, step = 2701 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1066.66\n",
      "INFO:tensorflow:loss = 1.51458, step = 2801 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1107.31\n",
      "INFO:tensorflow:loss = 1.51458, step = 2901 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1035.74\n",
      "INFO:tensorflow:loss = 1.51458, step = 3001 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1062.14\n",
      "INFO:tensorflow:loss = 1.51457, step = 3101 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1016.4\n",
      "INFO:tensorflow:loss = 1.51457, step = 3201 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.575\n",
      "INFO:tensorflow:loss = 1.51456, step = 3301 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 880.705\n",
      "INFO:tensorflow:loss = 1.51456, step = 3401 (0.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 1003.15\n",
      "INFO:tensorflow:loss = 1.51455, step = 3501 (0.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1056.19\n",
      "INFO:tensorflow:loss = 1.51455, step = 3601 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1140.98\n",
      "INFO:tensorflow:loss = 1.51455, step = 3701 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1086.73\n",
      "INFO:tensorflow:loss = 1.51454, step = 3801 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1102.31\n",
      "INFO:tensorflow:loss = 1.51454, step = 3901 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1116.99\n",
      "INFO:tensorflow:loss = 1.51453, step = 4001 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1129.79\n",
      "INFO:tensorflow:loss = 1.51453, step = 4101 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1176.69\n",
      "INFO:tensorflow:loss = 1.51452, step = 4201 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1147.73\n",
      "INFO:tensorflow:loss = 1.51452, step = 4301 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1136.43\n",
      "INFO:tensorflow:loss = 1.51452, step = 4401 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1098.61\n",
      "INFO:tensorflow:loss = 1.51451, step = 4501 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1053.24\n",
      "INFO:tensorflow:loss = 1.51451, step = 4601 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1208.67\n",
      "INFO:tensorflow:loss = 1.5145, step = 4701 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1290.67\n",
      "INFO:tensorflow:loss = 1.5145, step = 4801 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1240.33\n",
      "INFO:tensorflow:loss = 1.51449, step = 4901 (0.080 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /tmp/context_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.51449.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNRegressor(params={'head': <tensorflow.contrib.learn.python.learn.estimators.head._RegressionHead object at 0x1212b74e0>, 'hidden_units': [4, 4], 'feature_columns': (_RealValuedColumn(column_name='context_meta_id', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='date', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='day', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)), 'optimizer': None, 'activation_fn': <function relu at 0x11d0f9730>, 'dropout': None, 'gradient_clip_norm': None, 'embedding_lr_multipliers': None, 'input_layer_min_slice_size': None})"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "regressor.fit(input_fn=lambda: input_fn(training_set), steps=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-24-17:28:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/context_model/model.ckpt-5000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-24-17:28:34\n",
      "INFO:tensorflow:Saving dict for global step 5000: global_step = 5000, loss = 1.51449\n",
      "Loss: 1.514491\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Score accuracy\n",
    "ev = regressor.evaluate(input_fn=lambda: input_fn(test_set), steps=1)\n",
    "loss_score = ev[\"loss\"]\n",
    "print(\"Loss: {0:f}\".format(loss_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/akash/anaconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:347: calling DNNRegressor.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_scores, or set `outputs` argument.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/context_model/model.ckpt-5000\n",
      "Predictions: [2.1677377, 2.1605904, 2.1534452, 2.1512537, 2.2110114, 2.2137749]\n"
     ]
    }
   ],
   "source": [
    "# Print out predictions\n",
    "y = regressor.predict(input_fn=lambda: input_fn(prediction_set))\n",
    "# .predict() returns an iterator; convert to a list and print predictions\n",
    "predictions = list(itertools.islice(y, 6))\n",
    "print(\"Predictions: {}\".format(str(predictions)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
